{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fd703db",
   "metadata": {},
   "source": [
    "# <center>Tarefa 2</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a212b1",
   "metadata": {},
   "source": [
    "### 1. Cite 5 diferenças entre o AdaBoost e o GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4c2953",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>AdaBoost utiliza uma floresta de stumps, GBM utiliza um floresta de árvores</li>\n",
    "    <li>AdaBoost tem como seu primeiro passo um stump, o GBM, o primeiro passo é a média de Y</li>\n",
    "    <li>AdaBoost cada resposta tem um peso diferente, o GBM, todas as respostas das árvores possuem um multiplicador em comum chamada learning_rate</li>\n",
    "    <li>AdaBoost utiliza o Y predito, o GBM utiliza o calculo baseado em resíduos, ai sim utiliza o multiplicador para evitar o overfitting</li>\n",
    "    <li>AdaBoost sofre mais com outliers, já o GBM é mais robusto</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ee6d28",
   "metadata": {},
   "source": [
    "### 2. Acesse o link Scikit-learn – GBM, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27820c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X,y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42498ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b30746",
   "metadata": {},
   "source": [
    "### 3. Cite 5 Hyperparametros importantes no GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9809a5ba",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>learning_rate</li>\n",
    "    <li>n_estimators</li>\n",
    "    <li>min_samples_leaf</li>\n",
    "    <li>max_depth</li>\n",
    "    <li>verbose</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d111440e",
   "metadata": {},
   "source": [
    "### 4. (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b872eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08001688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, 1001, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbbb1c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 230, in _check_params\n",
      "    raise ValueError(\"n_estimators must be greater than 0 but \"\n",
      "ValueError: n_estimators must be greater than 0 but was 0\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 230, in _check_params\n",
      "    raise ValueError(\"n_estimators must be greater than 0 but \"\n",
      "ValueError: n_estimators must be greater than 0 but was 0\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 230, in _check_params\n",
      "    raise ValueError(\"n_estimators must be greater than 0 but \"\n",
      "ValueError: n_estimators must be greater than 0 but was 0\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 230, in _check_params\n",
      "    raise ValueError(\"n_estimators must be greater than 0 but \"\n",
      "ValueError: n_estimators must be greater than 0 but was 0\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 230, in _check_params\n",
      "    raise ValueError(\"n_estimators must be greater than 0 but \"\n",
      "ValueError: n_estimators must be greater than 0 but was 0\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Caio\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [   nan 0.8185 0.8605 0.881  0.891  0.8975 0.9065 0.9125 0.916  0.917\n",
      " 0.919 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingClassifier(),\n",
       "             param_grid={'learning_rate': [0.1], 'max_depth': [1],\n",
       "                         'n_estimators': [0, 100, 200, 300, 400, 500, 600, 700,\n",
       "                                          800, 900, 1000]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "params = {\n",
    "    'n_estimators' : list(range(0, 1001, 100)),\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [1]\n",
    "}\n",
    "\n",
    "grid_clf = GridSearchCV(estimator=clf,\n",
    "                       param_grid = params,\n",
    "                       scoring = 'accuracy',\n",
    "                       cv=5)\n",
    "\n",
    "grid_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2623fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 0}, {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 100}, {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 200}, {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 300}, {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 400}, {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500}, {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 600}, {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 700}, {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 800}, {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 900}, {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1000}]\n"
     ]
    }
   ],
   "source": [
    "print(grid_clf.cv_results_['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a778383d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_depth=1, n_estimators=1000)\n"
     ]
    }
   ],
   "source": [
    "print(grid_clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44822d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9203\n"
     ]
    }
   ],
   "source": [
    "print(grid_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb86c40",
   "metadata": {},
   "source": [
    "### 5. Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed0af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "em cada iteração, uma subamostra dos dados de treinamento é desenhada aleatoriamente (sem substituição) do conjunto de dados completo de treinamento. \n",
    "A subamostra selecionada aleatoriamente é então usada, em vez da amostra completa, para caber no algoritmo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
